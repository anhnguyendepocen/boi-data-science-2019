---
title: "A Typical (supervised) ML Workflow"
subtitle: ""
author: "Itamar Caspi"
date: " 10, 2019 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  eval = TRUE,
  echo = TRUE,
  warning = FALSE,
  message = FALSE,
  cache = FALSE,
  dev = "svglite",
  fig.ext = ".svg")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())
```


# Packages and setup

Use the [pacman](https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html) package that automatically loads and installs packages:

```{r pacman, message=FALSE, warning=FALSE, eval=TRUE}

if (!require("pacman")) install.packages("pacman")

pacman::p_load(
  tidyverse,   # for data wrangling and visualization
  tidymodels,  # for data modeling
  tune,        # for tuning hyperparameters
  GGally,      # for pairs plot
  naniar,      # for summary statistics
  here         # for referencing folders and files
  )


```

Set a theme for `ggplot` (Relevant only for the presentation)
```{r ggplot_theme}
theme_set(theme_grey(20))
```


---
# The `tidymodels` package

.pull-left[
```{r tidymodels_logo, echo=FALSE, out.width = "80%", fig.align='center'}
knitr::include_graphics("figs/tidymodels.png")
```
]
.pull-right[

>"[`tidymodels`](https://github.com/tidymodels/tidymodels) is a "meta-package" for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse."

]


---
# Supervised ML Workflow

Step 1: [Define the Prediton Task](#background)  

Step 2: [Explore the Data](#eda)  

Step 3: [Set Model and Tuning Parameters](#model)  

Step 4: [Cross-validation](#cv)  

Step 5: [Evaluate the Model](#eval)  


---
class: title-slide-section-blue, center, middle
name: background

# Step 1: Define the Prediction Task


---

# Predicting Boston Housing Prices

.pull-left[
We will use the `BostonHousing`: housing data for 506 census tracts of Boston from the 1970 census (Harrison and Rubinfeld, 1978).

- `medv` (target): median value of owner-occupied homes in USD 1000's.
- `lstat`(predictor): percentage of lower status of the
population.
- `chas` (predictor): Charles River dummy variable (= 1 if tract bounds river; 0 otherwise).

__OBJECTIVE:__ Predict `medv`.
]
.pull-right[
```{r boston_pic, echo=FALSE, out.width = "1000%", fig.align='center'}
knitr::include_graphics("figs/boston.jpg")
```
Source: [https://www.bostonusa.com/](https://www.bostonusa.com/)
]

---
# Load the Data

Laos the data
```{r load_data, message = TRUE}
boston_raw <- read_csv(here("04-ml-workflow/data","BostonHousing.csv"))
```

---
# What Type of Data?

We can use the `glimpse()` function in order to better understand the data structure:
```{r glimpse}
glimpse(boston_raw)
```

The `chas` variable is mostly zero $\Rightarrow$ should be a factor.

---
# Initial Data Filtering

Select `medv` and `lstat`
```{r filter_data}
boston <- boston_raw %>% 
  as_tibble() %>% 
  select(medv, lstat, chas) %>% 
  mutate(chas = as_factor(chas))

head(boston)
```



---
class: title-slide-section-blue, center, middle
name: split

# Step 2: Split the Data


---
# Initial Split 

We will use the `initial_split()`, `training()` and `testing()` functions from the [rsample](https://tidymodels.github.io/rsample/) package to perform an initial train-test split

Set seed for reproducibility
```{r seed}
set.seed(1203) 
```

Initial split:
```{r initial_split}
boston_split <- boston %>% 
  initial_split(prop = 2/3, strata = medv)

boston_split
```

---
# Prepare Training and Test Sets

```{r train_test_raw}
boston_train_raw <- training(boston_split)
boston_test_raw  <- testing(boston_split)

head(boston_train_raw, 5)
```


```{r head_test_raw}
head(boston_test_raw, 5)
```

---
class: title-slide-section-blue, center, middle
name: eda

# Step 3: Explore the Data

---
# Summary Statistics Using `skimr`

```{r skimr, eval=FALSE}
boston_train_raw %>% 
  skim() %>% partition()
```

(Does not come out well on these slides)

---
# Pairs Plot Using `GGally`

.pull-left[

We now use a __pairs plot__ which compactly plots every variable in a dataset against every other one.
```{r pairs, fig.width=6, fig.show='hide', fig.retina=3}
boston_train_raw %>% ggpairs()
```
]
.pull-right[
```{r, ref.label = 'pairs', echo=FALSE}

```
]
 
---
# Select a Model

.pull-left[

We choose the class of polynomial models:

$$medv_i = \beta_0 + \sum_{j=1}^{\lambda}\beta_j lstat_i^j+\varepsilon_i$$
```{r poly, fig.width=6, fig.show='hide', fig.retina=3}

boston_train_raw %>% ggplot(aes(lstat, medv)) +
  geom_point() +
  geom_smooth(
    method = lm,
    formula = y ~ poly(x,1),
    se = FALSE,
    color = "blue"
  ) +
  geom_smooth(
    method = lm,
    formula = y ~ poly(x,10),
    se = FALSE,
    color = "red"
  )
```
]

.pull-right[
In blue×ª $\lambda=1$; in red, $\lambda = 10$.
```{r, ref.label = 'poly', echo=FALSE}

```
]
 
 
---
class: title-slide-section-blue, center, middle
name: model

# Step 4: Set Model and Tuning Parameters


---

# Data Preprocessing using `recipes`

The `recipes` package is a great tool for data preprocessing that fits in naturally with the tidy approach to ML.

```{r recipe}
boston_rec <- 
  recipe(medv ~ lstat + chas, data = boston_train_raw) %>% 
  step_poly(lstat, degree = tune("lambda")) %>% 
  step_dummy(chas)

boston_rec
```


---
# Set a Grid for $\lambda$

What are our tuning parameters?

```{r params}
boston_rec %>% parameters()
```


We need to tune the polynomial degree parameter $(\lambda)$ when building our models on the train data. In this example, we will set the range between 1 and 8:
```{r grid}
lambda_grid <- expand_grid("lambda" = 1:8)
```

---
# Define the Model

We will use the linear regression model
```{r model}

lm_mod <- linear_reg()%>%
  set_engine("lm")

lm_mod
```
Note that there are no tuning parameters here.


---
class: title-slide-section-blue, center, middle
name: cv

# Step 5: Cross-validation


---
# Split the Training Set to 5-folds

We will use the `vfold-cv()` function from the [rsample](https://tidymodels.github.io/rsample/) package to split the training set to 5-folds:

```{r cv_split}
cv_splits <- boston_train_raw %>% 
  vfold_cv(v = 5)
  
cv_splits
```


---
# Estimate CV-RMSE Over the $\lambda$ Grid

We now estimate the CV-RMSE for each value of $\lambda$.
```{r tune}
boston_cv <- tune_grid(
  boston_rec,
  model = lm_mod,
  rs    = cv_splits,
  grid  = lambda_grid)

boston_cv
```

---
# Find the Optimal $\lambda$

Let's find the top-3 performing models
```{r rmse}
rmse_vals <- boston_cv %>% 
  estimate() %>% 
  filter(.metric == "rmse") %>% 
  arrange(mean)

head(rmse_vals, 3)
```

<midd-blockquote> _"[I]n reality there is rarely if ever a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts..."_ .right[&mdash; [__Rob J. Hyndman__](https://robjhyndman.com/hyndsight/crossvalidation/)] </midd-blockquote>

---
# And Now Using a Graph

.pull-left[
```{r cv_plot, echo=TRUE, fig.width=6, fig.show='hide', fig.retina=3}
rmse_vals %>% 
  ggplot(aes(x = lambda, y = mean)) + 
  geom_point() +
  geom_line() +
  geom_errorbar(
    aes(ymin = mean - std_err, ymax = mean + std_err),
    width = 0.2
  ) +
  labs(y = "CV-RMSE",
       x = "Lambda")
```
]
.pull-right[
```{r, ref.label = 'cv_plot', echo=FALSE}

```

]

---
class: title-slide-section-blue, center, middle
name: eval

# Step 6: Evaluate the Model


---

# Use the Test Set to Evaluate the Best Model

Prepare a recipe with the optimal $\lambda = 4$
```{r prep}
boston_prep <- 
  recipe(medv ~ lstat + chas, data = boston_train_raw) %>% 
  step_poly(lstat, degree = 4) %>%                #<<
  step_dummy(chas) %>% 
  prep()
```

---
# Apply the Recipe to the Training and Test Sets

`juice()` applies the recipe to the training set and `bake()` to the test set.
```{r test_train}
boston_train <- boston_prep %>% 
  juice()

boston_test <- boston_prep %>% 
  bake(new_data = boston_test_raw)
```

For example, let's take a look at the training set:
```{r head_train}
head(boston_train, 3)
```

---

# Fit the Model to the Training Set

Fit the optimal model $(\lambda = 4)$) to the training set:
```{r fit}
boston_fit <- lm_mod %>% 
  fit(medv ~ ., data = boston_train)
```

Here are the estimated coefficients:
```{r broom_fit}
boston_fit %>% tidy()
```


---
# Make Predictions Using the Test Set

Create a tibble with the predictions and ground-truth
```{r pred}
boston_pred <- boston_fit %>% 
  predict(new_data = boston_test) %>%   #<<
  bind_cols(boston_test) %>% 
  select(medv, .pred)

head(boston_pred)
```

Note that this is the first time we make use of the test set!

---
# Test-RMSE

Calculate the test root mean square error (test-RMSE):
```{r test_rmse}
boston_pred %>% 
  rmse(medv, .pred)
```


The above is a measure of our model's performance on "general" data.

<midd-blockquote>__NOTE:__ the test set RMSE estimates the expected squared prediction error on unseen data _given_ the best model.</midd-blockquote>

---
# Always plot your prediction errors

.pull-left[

Plotting the prediction errors $(y_i-\hat{y}_i)$ vs. the target provides valuable information about prediction quality.

```{r resid, fig.width=6, fig.show='hide', fig.retina=3}

boston_pred %>% 
  mutate(resid = medv - .pred) %>% 
  ggplot(aes(medv, resid)) +
  geom_point() +
  geom_hline(yintercept = 0, color = "red")

```

For example, our predictions for high-end levels of `medv` are extremely biased $\Rightarrow$ there's room for improvement...

]

.pull-right[
```{r, ref.label = 'resid', echo=FALSE}

```
]


---
class: .title-slide-final, center, inverse, middle

# `slides::end()`

[<i class="fa fa-github"></i> Source code](https://github.com/ml4econ/notes-spring2019/tree/master/04-mk-workflow)  
