---
title: "Typical (supervised) ML Workflow"
subtitle: ""
author: "Itamar Caspi"
date: " 10, 2019 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [default, "style/middlebury.css", "style/middlebury-fonts.css"]
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: "16:9"
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>
      
---

# Replicating this presentation

R packages used to produce this presentation

```{r load packages, message=FALSE}
library(tidyverse)  # for data wrangling and plotting
library(svglite)    # for better looking plots
library(kableExtra) # for better looking tables
library(tidymodels) # for modelling
library(knitr)      # for presenting tables
library(mlbench)    # for the Boston Housing data
```

If you are missing a package, run the following command

```
install.packages("package_name")

```

Alernatively, you can just use the [pacman](https://cran.r-project.org/web/packages/pacman/vignettes/Introduction_to_pacman.html) package that loads and installs packages:

```{r pacman, message=FALSE, warning=FALSE, eval=FALSE}

if (!require("pacman")) install.packages("pacman")

pacman::p_load(tidyverse, svglite, kabelExtra, 
               tidymodels, knitr, mlbench)


```

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(eval = TRUE,
               echo = TRUE,
               warning = FALSE,
               message = FALSE,
               cache = FALSE,
               dev = "svglite",
               fig.ext = ".svg")

htmltools::tagList(rmarkdown::html_dependency_font_awesome())

theme_set(theme_minimal(20))
```


---
class: title-slide-section-blue, center, middle
name: ml_workflow

# Putting it All Together


---

# Toy problem: Predicting Boston housing prices

We will use the `BostonHousing`: housing data for 506 census tracts of Boston from the 1970 census `r Citep(bib, "harrison1978hedonic")`

- `medv` (outcome): median value of owner-occupied homes in USD 1000's.
- `lstat`(predictor): percentage of lower status of the population.

__OBJECTIVE:__ Find the best prediction model within the class of polynomial regression.


```{r}
data("BostonHousing")

boston <- BostonHousing %>% 
  as_tibble() %>% 
  select(medv, lstat)
```


---
# Step 1: The train-test split

We will use the `initial_split()`, `training()` and `testing()` functions from the [rsample](https://tidymodels.github.io/rsample/) package to perform an initial train-test split

```{r}
set.seed(1203) # for reproducability

boston_split <- boston %>% 
  initial_split(prop = 0.5)

head(boston_split)
```


```{r}
boston_train <- training(boston_split)
boston_test  <- testing(boston_split)

head(boston_train, 5)
```


---

# Plot the training set


.pull-left[


Toy polynomial models: in green: linear relation $(\lambda=1)$; in blue, $(\lambda = 10)$.


```{r poly, echo=FALSE, fig.width=6, fig.show='hide', fig.retina=3}

boston_train %>% ggplot(aes(log(lstat), medv)) +
  geom_point() +
  geom_smooth(
    method = lm,
    formula = y ~ poly(x,1),
    se = FALSE,
    color = "green"
  ) +
  geom_smooth(
    method = lm,
    formula = y ~ poly(x,10),
    se = FALSE,
    color = "blue"
  )
```
]

.pull-right[
```{r, ref.label = 'poly', echo=FALSE}

```
]
 

---

# Preprocessing steps



---
# Step 2: Prepare 10 folds for cross-validation

We will use the `vfold-cv()` function from the [rsample](https://tidymodels.github.io/rsample/) package to split the training set to 10-folds:

```{r }
cv_data <- training_df %>% 
  vfold_cv(v = 10) %>%  
  mutate(train     = map(splits, ~training(.x)), 
         validate  = map(splits, ~testing(.x)))
cv_data
```

---
# Step 3: Set search range for lambda

We need to vary the polynomial degree parameter $(\lambda)$ when building our models on the train data. In this example, we will set the range between 1 and 10:

```{r }

cv_tune <- cv_data %>% 
  crossing(lambda = 1:10)

cv_tune
```

---
# Step 4: Estimate CV-MSE

We now estimate the CV-MSE for each value of $\lambda$.

```{r }

cv_mse <- cv_tune %>% 
  mutate(model = map2(lambda, train, ~ lm(medv ~ poly(lstat, .x), data = .y))) %>% 
  mutate(predicted = map2(model, validate, ~ augment(.x, newdata = .y))) %>% 
  unnest(predicted) %>% 
  group_by(lambda) %>% 
  summarise(mse = mean((.fitted - medv)^2))

cv_mse
```

---
# Step 5: Find the best model

Recall that the best performing model minimizes the CV-MSE.
 
```{r, echo=FALSE, out.width="25%", fig.align='center'}

cv_mse %>% 
  ggplot(aes(x = lambda, y = mse)) +
  geom_line() + 
  geom_point() +
  # geom_errorbar(aes(ymax = mse + sd, ymin = mse - sd)) +
  geom_vline(xintercept = 5, color = "blue") +
  labs(x = "Lambda", y = "10-fold CV MSE")

```

<midd-blockquote> _"[I]n reality there is rarely if ever a true underlying model, and even if there was a true underlying model, selecting that model will not necessarily give the best forecasts..."_ .right[&mdash; [__Rob J. Hyndman__](https://robjhyndman.com/hyndsight/crossvalidation/)] </midd-blockquote>

---

# Step 6: Use the test set to evaluate the best model


Fit the best model ( $\lambda = 5$) to the training set, make predictions on the test set, and calculate the test root mean square error (test-RMSE):

```{r, echo=FALSE, out.width = "50%", fig.align='center'}

include_graphics("figs/train_test.png")

```

```{r predict}

training_df %>% 
  lm(medv ~ poly(lstat, 5), data = .) %>%  # fit model
  augment(newdata = testing_df) %>%        # predict unseen data
  rmse(medv, .fitted)                  # evaluate accuracy
  
```


> __NOTE__: the test set RMSE is an estimator of the expected squared prediction error on unseen data _given_ the best model.

---
# An aside: plot your residuals

.pull-left[

The distribution of the prediction errors $(y_i-\hat{y}_i)$ are another important sources of information about prediction quality.

```{r resid, fig.width=6, fig.show='hide', fig.retina=3}

training_df %>% 
  lm(medv ~ poly(lstat, 5), data = .) %>% 
  augment(newdata = testing_df) %>%  
  mutate(error = medv - .fitted) %>% 
  ggplot(aes(medv, error)) +
  geom_point() +
  labs(y = expression(y[i] - hat(y)[i]))
  
```

For example, see how biased the prediction for high levels of `medv` are.

]

.pull-right[
```{r, ref.label = 'resid', echo=FALSE}

```
]


---
class: .title-slide-final, center, inverse, middle

# `slides::end()`

[<i class="fa fa-github"></i> Source code](https://github.com/ml4econ/notes-spring2019/tree/master/02-basic-ml-concepts)  
